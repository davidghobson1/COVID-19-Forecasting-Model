{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network Model for COVID-19 Data\n",
    "\n",
    "Employing an LSTM (Long Short Term Memory) Neural Network to forecast COVID-19 data as a time series. \n",
    "\n",
    "The neural network predicts confirmed cases, deaths, and recoveries related to COVID-19 for today based on the information from the past few days.\n",
    "\n",
    "The features, as well as the number of past days the network trains on, can all be adjusted.\n",
    "\n",
    "Time windows (specifically the `WindowGenerator` class) are based on this [TensorFlow tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data (both the competition data and the ESRI data of ICU and hospitalizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/train.csv')\n",
    "esri_df = pd.read_csv('Data/Provincial_Daily_Totals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "First clean the competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change date to datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Convert dates to the day of the year\n",
    "df['Day_of_Year'] = df['Date'].apply(lambda x: x.dayofyear)\n",
    "\n",
    "# Day of the week\n",
    "df[\"Day_of_Week\"] = df[\"Date\"].apply(lambda x: x.dayofweek)\n",
    "\n",
    "# Drop some of the columns\n",
    "df.drop(columns=['Id', 'Lat', 'Long', 'Testing_Info'], inplace=True)\n",
    "\n",
    "# Drop data for repatriated Canadians repatriated data\n",
    "df.drop(df[df['Province'] == 'Repatriated'].index, inplace=True)\n",
    "\n",
    "# Fill N/A values to 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Set index to Date\n",
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data for all provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "df.groupby(['Date','Province']).mean()['# Confirmed_Cases'].unstack().plot(ax=ax)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Deaths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the ESRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the data for Canada and Repatriated Canadians\n",
    "esri_df.drop(esri_df[esri_df['Abbreviation'].isin(['CA', 'RC'])].index, inplace=True)\n",
    "\n",
    "# Change the province names to be consistent with the Competition dataset\n",
    "esri_df.loc[esri_df['Province'] == 'ALBERTA', ['Province']] = 'Alberta'\n",
    "esri_df.loc[esri_df['Province'] == 'BRITISH COLUMBIA', ['Province']] = 'BC'\n",
    "esri_df.loc[esri_df['Province'] == 'MANITOBA', ['Province']] = 'Manitoba'\n",
    "esri_df.loc[esri_df['Province'] == 'NEW BRUNSWICK', ['Province']] = 'New Brunswick'\n",
    "esri_df.loc[esri_df['Province'] == 'NEWFOUNDLAND AND LABRADOR', ['Province']] = 'NL'\n",
    "esri_df.loc[esri_df['Province'] == 'NORTHWEST TERRITORIES', ['Province']] = 'NWT'\n",
    "esri_df.loc[esri_df['Province'] == 'NOVA SCOTIA', ['Province']] = 'Nova Scotia'\n",
    "esri_df.loc[esri_df['Province'] == 'NUNAVUT', ['Province']] = 'Nunavut'\n",
    "esri_df.loc[esri_df['Province'] == 'ONTARIO', ['Province']] = 'Ontario'\n",
    "esri_df.loc[esri_df['Province'] == 'PRINCE EDWARD ISLAND', ['Province']] = 'PEI'\n",
    "esri_df.loc[esri_df['Province'] == 'QUEBEC', ['Province']] = 'Quebec'\n",
    "esri_df.loc[esri_df['Province'] == 'SASKATCHEWAN', ['Province']] = 'Saskatchewan'\n",
    "esri_df.loc[esri_df['Province'] == 'YUKON', ['Province']] = 'Yukon'\n",
    "\n",
    "# Fill the N/A values\n",
    "esri_df = esri_df.fillna(0)\n",
    "\n",
    "# Change the Date to the Day of the Year\n",
    "esri_df['Date'] = pd.to_datetime(esri_df['SummaryDate'])\n",
    "esri_df['Day_of_Year'] = esri_df['Date'].apply(lambda x: x.dayofyear)\n",
    "\n",
    "# Drop some of the unecessary columns\n",
    "esri_df.drop(columns=['OBJECTID', 'SummaryDate', 'Date', 'DailyTested', 'DailyTotals', 'DailyRecovered', 'DailyDeaths', 'Abbreviation', 'TotalCases', 'TotalRecovered', 'TotalDeaths', 'TotalTested'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the ICU and Hospitalization data into the competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(esri_df, on=['Province', 'Day_of_Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data for Ontario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Province'] == 'Ontario']\n",
    "df.drop(columns=['Province', 'Population'], inplace=True)\n",
    "df.set_index('Day_of_Year', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of features and sort the column names by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.shape[1]\n",
    "column_indices = {name: i for i, name in enumerate(df.columns)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train, validation, and test sets (70%, 20%, 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df.shape[0]\n",
    "train_df = df[0:round(n*0.7)]\n",
    "valid_df = df[round(n*0.7):round(n*0.9)]\n",
    "test_df = df[round(n*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df-train_mean)/train_std\n",
    "valid_df = (valid_df-train_mean)/train_std\n",
    "test_df = (test_df-train_mean)/train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_std = (df - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(df.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WindowGenerator Class\n",
    "\n",
    "Create a class to represent time windows for training the neural networks.\n",
    "\n",
    "All code in this section is from https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, valid_df=valid_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.valid_df = valid_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to split a time window into a window of features and a window of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a plot function for the windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='# Deaths', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [days]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to take a DataFrame and convert it into a tf.data.Dataset object of the\n",
    "form (input_window, label_window) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=16,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add properties to the WindowGenerator objects to access the training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.valid_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['# Deaths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single time-step window, using the results from yesterday to predict the results today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=labels)\n",
    "\n",
    "# Output the dimensions\n",
    "for example_inputs, example_labels in single_step_window.train.take(1):\n",
    "  print(f'Inputs shape (batch, time-steps, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time-steps, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to store the performance measurements for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Baseline Model\n",
    "\n",
    "Assume what happens today is what happened yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model using the single step window\n",
    "baseline = Baseline(label_index=column_indices['# Deaths'])\n",
    "\n",
    "baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# evaluate the baseline model on the validation and test sets\n",
    "val_performance['Baseline'] = baseline.evaluate(single_step_window.val)\n",
    "performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the predictions over a 10 day period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    input_width=10, label_width=10, shift=1,\n",
    "    label_columns=labels)\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape (batch, time-steps, features):', wide_window.example[0].shape)\n",
    "print('Output shape (batch, time-steps, features):', baseline(wide_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = Sequential([\n",
    "    Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', single_step_window.example[0].shape)\n",
    "print('Output shape:', linear_model(single_step_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a maximum of 50 epochs\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "# Define a function to compile and fit the models\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = compile_and_fit(linear_model, single_step_window)\n",
    "\n",
    "val_performance['Linear'] = linear_model.evaluate(single_step_window.val)\n",
    "performance['Linear'] = linear_model.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x = range(len(train_df.columns)),\n",
    "        height=linear_model.layers[0].kernel[:,0].numpy())\n",
    "axis = plt.gca()\n",
    "axis.set_xticks(range(len(train_df.columns)))\n",
    "_ = axis.set_xticklabels(train_df.columns, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_model = Sequential([\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(dense_model, single_step_window)\n",
    "\n",
    "val_performance['Dense'] = dense_model.evaluate(single_step_window.val)\n",
    "performance['Dense'] = dense_model.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(dense_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Multi-step Dense Model \n",
    "\n",
    "Use data from multiple steps backwards to predict the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data from m days ago to predict the value on the m^th day\n",
    "m = 10\n",
    "\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=m,\n",
    "    label_width=1,\n",
    "    shift=1,\n",
    "    label_columns=labels)\n",
    "\n",
    "conv_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_window.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_dense_model = Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=1),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "    tf.keras.layers.Reshape([1, -1]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', conv_window.example[0].shape)\n",
    "print('Output shape:', multi_step_dense_model(conv_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = compile_and_fit(multi_step_dense_model, conv_window)\n",
    "\n",
    "val_performance['Multi step dense'] = multi_step_dense_model.evaluate(conv_window.val)\n",
    "performance['Multi step dense'] = multi_step_dense_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_window.plot(multi_step_dense_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,\n",
    "                           kernel_size=(m,),\n",
    "                           activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Conv model on `conv_window`\")\n",
    "print('Input shape:', conv_window.example[0].shape)\n",
    "print('Output shape:', conv_model(conv_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wide window\")\n",
    "print('Input shape:', wide_window.example[0].shape)\n",
    "print('Labels shape:', wide_window.example[1].shape)\n",
    "print('Output shape:', conv_model(wide_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot, we need the labels and the predictions to have the same length.\n",
    "Therefore, build a WindowGenerator to produce wide windows with a few extra inpout time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_WIDTH = 10\n",
    "INPUT_WIDTH = LABEL_WIDTH + (m - 1)\n",
    "wide_conv_window = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH,\n",
    "    label_width=LABEL_WIDTH,\n",
    "    shift=1,\n",
    "    label_columns=labels)\n",
    "wide_conv_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wide conv window\")\n",
    "print('Input shape:', wide_conv_window.example[0].shape)\n",
    "print('Labels shape:', wide_conv_window.example[1].shape)\n",
    "print('Output shape:', conv_model(wide_conv_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_conv_window.plot(conv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Recurrent Neural Network (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    # Shape => [batch, time, features]\n",
    "    Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', wide_window.example[0].shape)\n",
    "print('Labels shape:', wide_window.example[1].shape)\n",
    "print('Output shape:', lstm_model(wide_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = compile_and_fit(lstm_model, wide_window)\n",
    "\n",
    "val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\n",
    "performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance and comparison of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(performance))\n",
    "width = 0.3\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = lstm_model.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in performance.values()]\n",
    "\n",
    "plt.ylabel('mean_absolute_error [# Deaths, normalized]')\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=performance.keys(),\n",
    "           rotation=45)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in performance.items():\n",
    "  print(f'{name:12s}: {value[1]:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Multi Output Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10 days of data to predict all features for the next 10 days (still single-time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window_2 = WindowGenerator(\n",
    "    input_width=10, label_width=10, shift=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_2 = Baseline()\n",
    "baseline_2.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance_2 = {}\n",
    "performance_2 = {}\n",
    "val_performance_2['Baseline'] = baseline_2.evaluate(wide_window_2.val)\n",
    "performance_2['Baseline'] = baseline_2.evaluate(wide_window_2.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window_2.plot(baseline_2, plot_col='# Deaths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    Dense(units=num_features)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(lstm_model, wide_window_2)\n",
    "\n",
    "val_performance['LSTM'] = lstm_model.evaluate( wide_window_2.val)\n",
    "performance['LSTM'] = lstm_model.evaluate( wide_window_2.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a plot of the predicted # Tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window_2.plot(lstm_model, plot_col='# Tested')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Multi-step Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models before only predict one or several features just one time unit into the future.\n",
    "\n",
    "This will treat the case of predicting several units into the future (multiple timestep predictions).\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "1) Single shot predictions where the entire time series is predicted at once. (This is what we do here.)\n",
    "\n",
    "2) Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.\n",
    "\n",
    "Here, all the models will predict all the features across all output time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Window object for the multi-step output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_forecast_days = 7\n",
    "multi_window = WindowGenerator(input_width=7,\n",
    "                               label_width=num_forecast_days,\n",
    "                               shift=num_forecast_days)\n",
    "\n",
    "multi_window.plot(plot_col='# Deaths', max_subplots=3)\n",
    "multi_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7a) Multi-Step Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One approach is to just use the last seen value repeatedly\n",
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return tf.tile(inputs[:, -1:, :], [1, num_forecast_days, 1])\n",
    "\n",
    "last_baseline = MultiStepLastBaseline()\n",
    "last_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "\n",
    "multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(last_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another baseline approach is to just repeat the pattern that you just saw\n",
    "class RepeatBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return inputs\n",
    "\n",
    "repeat_baseline = RepeatBaseline()\n",
    "repeat_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(repeat_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7b) Single Shot Models\n",
    "\n",
    "These models take the previous values, and use them to predict all of the output labels at once\n",
    "without any autoregressive behaviour (i.e. adding the previous prediction as input to the make \n",
    "the next prediction)\n",
    "\n",
    "(See the TensorFlow time series tutorial in the Advanced section at the bottom, for an implementation of this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_linear_model = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, num_forecast_days*features]\n",
    "    tf.keras.layers.Dense(num_forecast_days*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, num_forecast_days, features]\n",
    "    tf.keras.layers.Reshape([num_forecast_days, num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_linear_model, multi_window)\n",
    "\n",
    "multi_val_performance['Linear_multi'] = multi_linear_model.evaluate(multi_window.val)\n",
    "multi_performance['Linear_multi'] = multi_linear_model.evaluate(multi_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot(multi_linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_dense_model = tf.keras.Sequential([\n",
    "    # Take the last time step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, dense_units]\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Shape => [batch, num_forecast_days*features]\n",
    "    tf.keras.layers.Dense(num_forecast_days*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, num_forecast_days, features]\n",
    "    tf.keras.layers.Reshape([num_forecast_days, num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_dense_model, multi_window)\n",
    "\n",
    "multi_val_performance['Dense_multi'] = multi_dense_model.evaluate(multi_window.val)\n",
    "multi_performance['Dense_multi'] = multi_dense_model.evaluate(multi_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot(multi_dense_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model\n",
    "\n",
    "A recurrent model can learn to use a long history of inputs, if it's relevant to the predictions the model is making. Here the model will accumulate internal state for 7 days, before making a single prediction for the next 7 days.\n",
    "\n",
    "In this single-shot format, the LSTM only needs to produce an output at the last time step, so set return_sequences=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    # Shape => [batch, num_forecast_days*features]\n",
    "    tf.keras.layers.Dense(num_forecast_days*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, num_forecast_days, features]\n",
    "    tf.keras.layers.Reshape([num_forecast_days, num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_lstm_model, multi_window)\n",
    "\n",
    "multi_val_performance['LSTM_multi'] = multi_lstm_model.evaluate(multi_window.val)\n",
    "multi_performance['LSTM_multi'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot(multi_lstm_model, plot_col='# Deaths')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
